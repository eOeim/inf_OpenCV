{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.5.2 |Anaconda 4.2.0 (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n",
      "opencv: 3.1.0\n",
      "pytesseract: pytesseract version 0.1.6\n"
     ]
    }
   ],
   "source": [
    "# OpenCV 및 OCR모듈 설치 유무 확인\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# 윈도우에서 주석해제 (tesseract_path: tesseract설치경로 확인 후 붙여넣기)\n",
    "# tesseract_path = 'C:/Program Files (x86)/Tesseract-OCR'\n",
    "# pytesseract.pytesseract.tesseract_cmd = tesseract_path + '/tesseract'\n",
    "\n",
    "print (\"python:\", sys.version)\n",
    "print (\"opencv:\", cv2.__version__)\n",
    "print (\"pytesseract:\", pytesseract.image_to_string(Image.open('images/test.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV - 이미지 읽기, 쓰기 및 표시하기 (1)\n",
    "\n",
    "# import numpy as np // 고성능 수치 계산\n",
    "import cv2\n",
    "\n",
    "def handle_image():\n",
    "    imgfile = 'images/sample.png'\n",
    "    img = cv2.imread(imgfile, cv2.IMREAD_COLOR)\n",
    "    # 매개인자 1 : 파일 경로, 2 : 읽는 방식 (컬러이미지로 불러온다,)\n",
    "    \n",
    "    cv2.imshow('SongNamJoo', img)\n",
    "    # 이미지 출력\n",
    "    # 매개인자 1 : 이미지 타이틀, 2 : 불러올 이미지\n",
    "    cv2.waitKey(0)\n",
    "    # 사용자가 키보드 누를때 까지 대기\n",
    "    # 매개인자 : sec 단위 키보드 입력 대기, 0은 무한정 기다린다.\n",
    "    # 창이 띄워진 다음 로직 실행하지 않고 다음 무한정 기다림\n",
    "    cv2.destroyAllWindows()\n",
    "    # (아무 키나 누르면) 표시한 모든 윈도우 닫음\n",
    "    cv2.waitKey(1)\n",
    "    # 없어도 되지만, 쥬피터 노트북 버그로 인해 라인 하나 더 추가\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    handle_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV - 이미지 읽기, 쓰기 및 표시하기 (2)\n",
    "\n",
    "import cv2\n",
    "\n",
    "def handle_image():\n",
    "    imgfile = 'images/sample.png'\n",
    "    img = cv2.imread(imgfile, cv2.IMREAD_GRAYSCALE)\n",
    "    # cv2.IMREAD_GRAYSCALE : 회색으로 이미시 표시\n",
    "    \n",
    "    cv2.namedWindow('image', cv2.WINDOW_NORMAL)\n",
    "    # 이미지 띄울 창에 다양한 속성 부여\n",
    "    # 매개인자 1 : 윈도우 타이틀\n",
    "    # cv2.WINDOW_NORMAL : 원본 이미지 크기, 사이즈 조정가능\n",
    "    # cs2.WINDOW_AUTOSIZE : 원본 이미지 그대로\n",
    "    \n",
    "    cv2.imshow('image', img)\n",
    "    k = cv2.waitKey(0)\n",
    "    # k = 아스키 코드값\n",
    "    if k == 27:\n",
    "    # 27은 [ESC] 의미\n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.waitKey(1)\n",
    "    elif k == ord('s'):\n",
    "    # 's' 일경우 저장 후 닫기\n",
    "        cv2.imwrite('grayImage.png', img)\n",
    "        # 매개인자 1 : 파일경로, 매개인자 2 : 저장할 객체\n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    handle_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV - 도형 외곽 추출하기 (1)\n",
    "\n",
    "import cv2\n",
    "\n",
    "def contour():\n",
    "    imgfile = 'images/contour.jpg'\n",
    "    img = cv2.imread(imgfile)\n",
    "    # 이미지 경로 읽어 변수에 저장\n",
    "    imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # 이미지 색공간을 바꾸는 함수\n",
    "    # 파라미터 1 : 색공간 바꿀 객체, 파라미터 2 : 어떤 색공간으로 바꿀지 지정\n",
    "    # cv2.COLOR_BGR2GRAY : BGR 에서 GRAY로 바꾸겠다. BGR = RGB\n",
    "    \n",
    "    # img는 원본 칼라, imgray는 흑백 이미지\n",
    "    # HSV Color Space : 특정한 색상 영역 추출 (예 : 조명, 질감에 따라 다른 빨간색을 포괄하는 지정 영역)\n",
    "    #  Hue : 색상 값 0~10도 얼추 빨간색\n",
    "    #  Saturation : 진함의 정도\n",
    "    #  Value : 밝은 정도 (검은색 0, 흰색 100)\n",
    "    # RGB Color Space : 이미지 저장, 불러오기 (3가지 숫자 좌표 불러오기 편함)\n",
    "    \n",
    "    edge = cv2.Canny(imgray, 100, 200)\n",
    "    # cv.Canny : edge 찾기 알고리즘\n",
    "    # 매개인자 1 : 객체, 2 : 해당 값보다 작으면 엣지 X, 3 : 해당 값보다 크면 확실한 엣지 O\n",
    "    # 그 사이 값은 ? 엣지 간의 연결성 통해 엣지 여부 판단\n",
    "    # 우리가 할 일 ) 적당한 값을 찾아야 한다!\n",
    "    \n",
    "    edge, contours, hierarchy = cv2.findContours(edge, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # contours : 외곽을 나타내는 배열\n",
    "    # hierarchy : 외곽간의 관계 (부모, 형제)\n",
    "    # 넘기는 이미지는 검은색과 흰색 (이진화)으로만 되어 있어야 한다.\n",
    "    # 배경은 검은색, 외곽은 흰색\n",
    "    # 매개인자 2,3은 외곽을 찾는 방식\n",
    "    # cv2.RETR_TREE : 외곽 관계를 트리형태\n",
    "    # cv2.CHAIN_APPROX_SIMPLE : 꼭지점만 반환 할 지 모두 반환 할지 설정\n",
    "    \n",
    "    cv2.imshow('gray', imgray)\n",
    "    cv2.imshow('edge', edge)\n",
    "    \n",
    "    cv2.drawContours(img, contours, -1, (255, 0, 0), 1)\n",
    "    # 원본 이미지 위에 contours를 그림\n",
    "    # 매개인자 1 : 외곽 그릴 이미지 객체\n",
    "    # 매개인자 2 : findContours를 통해 찾은 contours 객체\n",
    "    # 매개인자 3 : contours의 Index : 모두 그리고 싶으면 -1, 아니면 특정한 index 지정\n",
    "    # 원하는 index가 어디 있을까? 알고리즘이 탐색한 순서 (변칙적인 순서)\n",
    "    # contours 배열에서 정렬이 필요함 (면적 큰 순서 대로)\n",
    "    # 매개인자 4 : 그릴 contours Line의 색상 (BGR)\n",
    "    # 매개인자 5 : 그릴 contours Line의 두께\n",
    "    \n",
    "    cv2.imshow('Contour', img)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    contour() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV - 도형 외곽 추출하기 (2)\n",
    "\n",
    "import cv2\n",
    "\n",
    "def contour_approx():\n",
    "    imgfile = 'images/contour2.png'\n",
    "    img = cv2.imread(imgfile)\n",
    "    img2 = img.copy()\n",
    "    imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    edge = cv2.Canny(imgray, 100, 200)\n",
    "    edge, contours, hierarchy = cv2.findContours(edge, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    cnt = contours[0]\n",
    "    cv2.drawContours(img, [cnt], 0, (0, 255, 0), 3)\n",
    "    \n",
    "    epsilon = 0.1 * cv2.arcLength(cnt, True)\n",
    "    \n",
    "    approx = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "    \n",
    "    cv2.drawContours(img2, [approx], 0, (0, 255, 0), 3)\n",
    "    \n",
    "    cv2.imshow('Contour', img)\n",
    "    cv2.imshow('Approx', img2)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    contour_approx() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV - 투영변환 구현하기 (1)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def warp_affine():\n",
    "    img = cv2.imread('images/transform.png')\n",
    "    \n",
    "    pts1 = np.float32([[50, 50], [200, 50], [20, 200]])\n",
    "    pts2 = np.float32([[70, 100], [220, 50], [150, 250]])\n",
    "    \n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "    \n",
    "    result = cv2.warpAffine(img, M, (350, 300))\n",
    "    \n",
    "    cv2.imshow('original', img)\n",
    "    cv2.imshow('Affine Transform', result)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    warp_affine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV - 투영변환 구현하기 (2)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def warp_perspective():\n",
    "    img = cv2.imread('images/transform.jpg')\n",
    "    \n",
    "    topLeft = [127, 157]\n",
    "    topRight = [448, 152]\n",
    "    bottomRight = [579, 526]\n",
    "    bottomLeft = [54, 549]\n",
    "    \n",
    "    pts1 = np.float32([topLeft, topRight, bottomRight, bottomLeft])\n",
    "    \n",
    "    w1 = abs(bottomRight[0] - bottomLeft[0])\n",
    "    w2 = abs(topRight[0] - topLeft[0])\n",
    "    h1 = abs(topRight[1] - bottomRight[1])\n",
    "    h2 = abs(topLeft[1] - bottomLeft[1])\n",
    "    minWidth = min([w1, w2])\n",
    "    minHeight = min([h1, h2])\n",
    "    \n",
    "    pts2 = np.float32([[0,0], [minWidth-1,0], \n",
    "                      [minWidth-1,minHeight-1], [0,minHeight-1]])\n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "    \n",
    "    result = cv2.warpPerspective(img, M, (int(minWidth), int(minHeight)))\n",
    "    \n",
    "    cv2.imshow('original', img)\n",
    "    cv2.imshow('Warp Transform', result)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    warp_perspective()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV - 스캔한 듯한 효과 주기 (1)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Callback Function for Trackbar (but do not any work)\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "def global_threshold():\n",
    "    imgfile = 'images/document.jpg'\n",
    "    img = cv2.imread(imgfile, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Resize image\n",
    "    r = 600.0 / img.shape[0]\n",
    "    dim = (int(img.shape[1] * r), 600)\n",
    "    img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    WindowName = \"Window\"\n",
    "    TrackbarName = \"Threshold\"\n",
    "    \n",
    "    # Make Window and Trackbar\n",
    "    cv2.namedWindow(WindowName)\n",
    "    cv2.createTrackbar(TrackbarName, WindowName, 70, 255, nothing)\n",
    "    \n",
    "    # Allocate destination image\n",
    "    Threshold = np.zeros(img.shape, np.uint8)\n",
    "    \n",
    "    # Loop for get trackbar pos and process it\n",
    "    while True:\n",
    "        # Get position in trackbar\n",
    "        TrackbarPos = cv2.getTrackbarPos(TrackbarName, WindowName)\n",
    "        # Apply threshold\n",
    "        cv2.threshold(img, TrackbarPos, 255, cv2.THRESH_BINARY, Threshold)\n",
    "        # Show in window\n",
    "        cv2.imshow(WindowName, Threshold)\n",
    "        \n",
    "        # wait for ESC key to exit\n",
    "        k = cv2.waitKey(0)\n",
    "        if k == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            cv2.waitKey(1)\n",
    "            break\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    global_threshold() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV - 스캔한 듯한 효과 주기 (2)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def adaptive_threshold():\n",
    "    imgfile = 'images/document.jpg'\n",
    "    img = cv2.imread(imgfile, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Resize image\n",
    "    r = 600.0 / img.shape[0]\n",
    "    dim = (int(img.shape[1] * r), 600)\n",
    "    img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    # Blur image and apply adaptive threshold\n",
    "    blur = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    result_without_blur = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "    result_with_blur = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "    cv2.imshow('Without Blur', result_without_blur)\n",
    "    cv2.imshow('With Blur', result_with_blur)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    adaptive_threshold() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Edge Detection\n",
      "STEP 2: Find contours of paper\n",
      "STEP 3: Apply perspective transform\n",
      "STEP 4: Apply Adaptive Threshold\n"
     ]
    }
   ],
   "source": [
    "# 명함인식 구현하기 - 캡처된 이미지\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def order_points(pts):\n",
    "    # initialzie a list of coordinates that will be ordered\n",
    "    # such that the first entry in the list is the top-left,\n",
    "    # the second entry is the top-right, the third is the\n",
    "    # bottom-right, and the fourth is the bottom-left\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "\n",
    "    # the top-left point will have the smallest sum, whereas\n",
    "    # the bottom-right point will have the largest sum\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "\n",
    "    # now, compute the difference between the points, the\n",
    "    # top-right point will have the smallest difference,\n",
    "    # whereas the bottom-left will have the largest difference\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "\n",
    "    # return the ordered coordinates\n",
    "    return rect\n",
    "\n",
    "def auto_scan_image():\n",
    "    # load the image and compute the ratio of the old height\n",
    "    # to the new height, clone it, and resize it\n",
    "    # document.jpg ~ docuemnt7.jpg\n",
    "    image = cv2.imread('images/document.jpg')\n",
    "    orig = image.copy()\n",
    "    r = 800.0 / image.shape[0]\n",
    "    dim = (int(image.shape[1] * r), 800)\n",
    "    image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    # convert the image to grayscale, blur it, and find edges\n",
    "    # in the image\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    edged = cv2.Canny(gray, 75, 200)\n",
    "\n",
    "    # show the original image and the edge detected image\n",
    "    print (\"STEP 1: Edge Detection\")\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.imshow(\"Edged\", edged)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    # find the contours in the edged image, keeping only the\n",
    "    # largest ones, and initialize the screen contour\n",
    "    (_, cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "\n",
    "    # loop over the contours\n",
    "    for c in cnts:\n",
    "        # approximate the contour\n",
    "        peri = cv2.arcLength(c, True)\n",
    "        approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "\n",
    "        # if our approximated contour has four points, then we\n",
    "        # can assume that we have found our screen\n",
    "        if len(approx) == 4:\n",
    "            screenCnt = approx\n",
    "            break\n",
    "\n",
    "    # show the contour (outline) of the piece of paper\n",
    "    print (\"STEP 2: Find contours of paper\")\n",
    "    cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Outline\", image)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    # apply the four point transform to obtain a top-down\n",
    "    # view of the original image\n",
    "    rect = order_points(screenCnt.reshape(4, 2) / r)\n",
    "    (topLeft, topRight, bottomRight, bottomLeft) = rect\n",
    "    \n",
    "    w1 = abs(bottomRight[0] - bottomLeft[0])\n",
    "    w2 = abs(topRight[0] - topLeft[0])\n",
    "    h1 = abs(topRight[1] - bottomRight[1])\n",
    "    h2 = abs(topLeft[1] - bottomLeft[1])\n",
    "    maxWidth = max([w1, w2])\n",
    "    maxHeight = max([h1, h2])\n",
    "    \n",
    "    dst = np.float32([[0,0], [maxWidth-1,0], \n",
    "                      [maxWidth-1,maxHeight-1], [0,maxHeight-1]])\n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warped = cv2.warpPerspective(orig, M, (maxWidth, maxHeight))\n",
    "\n",
    "    # show the original and scanned images\n",
    "    print (\"STEP 3: Apply perspective transform\")\n",
    "    cv2.imshow(\"Warped\", warped)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "    # convert the warped image to grayscale, then threshold it\n",
    "    # to give it that 'black and white' paper effect\n",
    "    warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "    warped = cv2.adaptiveThreshold(warped, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "\n",
    "    # show the original and scanned images\n",
    "    print (\"STEP 4: Apply Adaptive Threshold\")\n",
    "    cv2.imshow(\"Original\", orig)\n",
    "    cv2.imshow(\"Scanned\", warped)\n",
    "    cv2.imwrite('scannedImage.png', warped)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    auto_scan_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "306.0\n",
      "921600\n",
      "0.00033203125\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "1847.5\n",
      "921600\n",
      "0.002004665798611111\n",
      "STEP 1: Edge Detection\n",
      "38.5\n",
      "921600\n",
      "4.177517361111111e-05\n",
      "STEP 1: Edge Detection\n",
      "310.0\n",
      "921600\n",
      "0.0003363715277777778\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "520.0\n",
      "921600\n",
      "0.0005642361111111111\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "1054.0\n",
      "921600\n",
      "0.0011436631944444445\n",
      "STEP 1: Edge Detection\n",
      "86.5\n",
      "921600\n",
      "9.385850694444444e-05\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n"
     ]
    }
   ],
   "source": [
    "# 명함인식 구현하기 - 웹캠(1)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def auto_scan_image_via_webcam():\n",
    "    \n",
    "    try: \n",
    "        cap = cv2.VideoCapture(0)\n",
    "    except:\n",
    "        print ('cannot load camera!')\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print ('cannot load camera!')\n",
    "            break\n",
    "            \n",
    "        k = cv2.waitKey(10)\n",
    "        if k == 27:\n",
    "            break\n",
    "        \n",
    "        # convert the image to grayscale, blur it, and find edges\n",
    "        # in the image\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        edged = cv2.Canny(gray, 75, 200)\n",
    "\n",
    "        # show the original image and the edge detected image\n",
    "        print (\"STEP 1: Edge Detection\")\n",
    "\n",
    "        # find the contours in the edged image, keeping only the\n",
    "        # largest ones, and initialize the screen contour\n",
    "        (_, cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "\n",
    "        # loop over the contours\n",
    "        for c in cnts:\n",
    "            # approximate the contour\n",
    "            peri = cv2.arcLength(c, True)\n",
    "            approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "            screenCnt = []\n",
    "\n",
    "            # if our approximated contour has four points, then we\n",
    "            # can assume that we have found our screen\n",
    "            if len(approx) == 4:\n",
    "                contourSize = cv2.contourArea(approx)\n",
    "                camSize = frame.shape[0] * frame.shape[1]\n",
    "                ratio = contourSize / camSize\n",
    "                print (contourSize)\n",
    "                print (camSize)\n",
    "                print (ratio)\n",
    "                \n",
    "                if ratio > 0.1:\n",
    "                    screenCnt = approx\n",
    "                    \n",
    "                break \n",
    "        \n",
    "        if len(screenCnt) == 0:\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            # show the contour (outline) of the piece of paper\n",
    "            print (\"STEP 2: Find contours of paper\")\n",
    "\n",
    "            cv2.drawContours(frame, [screenCnt], -1, (0, 255, 0), 2)\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "        \n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    auto_scan_image_via_webcam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Edge Detection\n",
      "30.5\n",
      "921600\n",
      "3.309461805555556e-05\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "23.5\n",
      "921600\n",
      "2.5499131944444446e-05\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "243.0\n",
      "921600\n",
      "0.000263671875\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "0.0\n",
      "921600\n",
      "0.0\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "172.0\n",
      "921600\n",
      "0.00018663194444444445\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "485.0\n",
      "921600\n",
      "0.0005262586805555555\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "392.0\n",
      "921600\n",
      "0.0004253472222222222\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "80.5\n",
      "921600\n",
      "8.734809027777777e-05\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "74.0\n",
      "921600\n",
      "8.029513888888889e-05\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "16.5\n",
      "921600\n",
      "1.7903645833333333e-05\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "172.0\n",
      "921600\n",
      "0.00018663194444444445\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "151.5\n",
      "921600\n",
      "0.00016438802083333333\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "112.5\n",
      "921600\n",
      "0.0001220703125\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "311.0\n",
      "921600\n",
      "0.0003374565972222222\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "253.0\n",
      "921600\n",
      "0.0002745225694444444\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "4.5\n",
      "921600\n",
      "4.8828125e-06\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "84.0\n",
      "921600\n",
      "9.114583333333334e-05\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "72035.0\n",
      "921600\n",
      "0.07816297743055556\n",
      "STEP 1: Edge Detection\n",
      "73675.0\n",
      "921600\n",
      "0.07994249131944445\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "78000.0\n",
      "921600\n",
      "0.08463541666666667\n",
      "STEP 1: Edge Detection\n",
      "80330.0\n",
      "921600\n",
      "0.08716362847222223\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "337.0\n",
      "921600\n",
      "0.0003656684027777778\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "149336.5\n",
      "921600\n",
      "0.16204047309027778\n",
      "STEP 2: Find contours of paper\n",
      "STEP 3: Apply perspective transform\n",
      "STEP 4: Apply Adaptive Threshold\n"
     ]
    }
   ],
   "source": [
    "# 명함인식 구현하기 - 웹캠(2)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def order_points(pts):\n",
    "    # initialzie a list of coordinates that will be ordered\n",
    "    # such that the first entry in the list is the top-left,\n",
    "    # the second entry is the top-right, the third is the\n",
    "    # bottom-right, and the fourth is the bottom-left\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "\n",
    "    # the top-left point will have the smallest sum, whereas\n",
    "    # the bottom-right point will have the largest sum\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "\n",
    "    # now, compute the difference between the points, the\n",
    "    # top-right point will have the smallest difference,\n",
    "    # whereas the bottom-left will have the largest difference\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "\n",
    "    # return the ordered coordinates\n",
    "    return rect\n",
    "\n",
    "def auto_scan_image_via_webcam():\n",
    "    \n",
    "    try: \n",
    "        cap = cv2.VideoCapture(0)\n",
    "    except:\n",
    "        print ('cannot load camera!')\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print ('cannot load camera!')\n",
    "            break\n",
    "            \n",
    "        k = cv2.waitKey(10)\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "        # convert the image to grayscale, blur it, and find edges\n",
    "        # in the image\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        edged = cv2.Canny(gray, 75, 200)\n",
    "\n",
    "        # show the original image and the edge detected image\n",
    "        print (\"STEP 1: Edge Detection\")\n",
    "\n",
    "        # find the contours in the edged image, keeping only the\n",
    "        # largest ones, and initialize the screen contour\n",
    "        (_, cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "\n",
    "        # loop over the contours\n",
    "        for c in cnts:\n",
    "            # approximate the contour\n",
    "            peri = cv2.arcLength(c, True)\n",
    "            approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "            screenCnt = []\n",
    "\n",
    "            # if our approximated contour has four points, then we\n",
    "            # can assume that we have found our screen\n",
    "            if len(approx) == 4:\n",
    "                contourSize = cv2.contourArea(approx)\n",
    "                camSize = frame.shape[0] * frame.shape[1]\n",
    "                ratio = contourSize / camSize\n",
    "                print (contourSize)\n",
    "                print (camSize)\n",
    "                print (ratio)\n",
    "                \n",
    "                if ratio > 0.1:\n",
    "                    screenCnt = approx\n",
    "                    \n",
    "                break \n",
    "        \n",
    "        if len(screenCnt) == 0:\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            # show the contour (outline) of the piece of paper\n",
    "            print (\"STEP 2: Find contours of paper\")\n",
    "\n",
    "            cv2.drawContours(frame, [screenCnt], -1, (0, 255, 0), 2)\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            \n",
    "            # apply the four point transform to obtain a top-down\n",
    "            # view of the original image\n",
    "            rect = order_points(screenCnt.reshape(4, 2))\n",
    "            (topLeft, topRight, bottomRight, bottomLeft) = rect\n",
    "\n",
    "            w1 = abs(bottomRight[0] - bottomLeft[0])\n",
    "            w2 = abs(topRight[0] - topLeft[0])\n",
    "            h1 = abs(topRight[1] - bottomRight[1])\n",
    "            h2 = abs(topLeft[1] - bottomLeft[1])\n",
    "            maxWidth = max([w1, w2])\n",
    "            maxHeight = max([h1, h2])\n",
    "\n",
    "            dst = np.float32([[0,0], [maxWidth-1,0], \n",
    "                              [maxWidth-1,maxHeight-1], [0,maxHeight-1]])\n",
    "\n",
    "            M = cv2.getPerspectiveTransform(rect, dst)\n",
    "            warped = cv2.warpPerspective(frame, M, (maxWidth, maxHeight))\n",
    "\n",
    "            # show the original and scanned images\n",
    "            print (\"STEP 3: Apply perspective transform\")\n",
    "\n",
    "            # convert the warped image to grayscale, then threshold it\n",
    "            # to give it that 'black and white' paper effect\n",
    "            warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "            warped = cv2.adaptiveThreshold(warped, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "\n",
    "            # show the original and scanned images\n",
    "            print (\"STEP 4: Apply Adaptive Threshold\")\n",
    "\n",
    "            break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "    cv2.imshow(\"Scanned\", warped)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    auto_scan_image_via_webcam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"“éi’LSﬂOid: $1113\n",
      "Special Coupon\n",
      "\n",
      "' PARKSEUNGCMOL\n",
      "HAIRSTUDIO\n"
     ]
    }
   ],
   "source": [
    "# OCR - Tesseract\n",
    "\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "def ocr_tesseract():\n",
    "    image_file = 'images/scannedImage.png'\n",
    "    im = Image.open(image_file)\n",
    "    text = pytesseract.image_to_string(im)\n",
    "    im.show()\n",
    "\n",
    "    print (text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ocr_tesseract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"language\":\"cs\",\"textAngle\":0.0,\"orientation\":\"Up\",\"regions\":[{\"boundingBox\":\"86,146,340,210\",\"lines\":[{\"boundingBox\":\"86,146,340,47\",\"words\":[{\"boundingBox\":\"86,146,155,46\",\"text\":\"Special\"},{\"boundingBox\":\"256,147,170,46\",\"text\":\"Coupon\"}]},{\"boundingBox\":\"167,324,186,13\",\"words\":[{\"boundingBox\":\"167,324,186,13\",\"text\":\"PARKSEUNGCHOL\"}]},{\"boundingBox\":\"196,343,128,13\",\"words\":[{\"boundingBox\":\"196,343,128,13\",\"text\":\"HAIRSTUDIO\"}]}]}]}\n",
      "\n",
      "Special Coupon\n",
      "PARKSEUNGCHOL\n",
      "HAIRSTUDIO\n"
     ]
    }
   ],
   "source": [
    "# OCR - Project Oxford by MS\n",
    "\n",
    "from PIL import Image\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64, json\n",
    "\n",
    "def print_text(json_data):\n",
    "    result = json.loads(json_data)\n",
    "    for l in result['regions']:\n",
    "        for w in l['lines']:\n",
    "            line = []\n",
    "            for r in w['words']:\n",
    "                line.append(r['text'])\n",
    "            print (' '.join(line))\n",
    "    return\n",
    "\n",
    "def ocr_project_oxford(headers, params, data):\n",
    "    conn = http.client.HTTPSConnection('westus.api.cognitive.microsoft.com')\n",
    "    conn.request(\"POST\", \"/vision/v1.0/ocr?%s\" % params, data, headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read().decode()\n",
    "    print (data + \"\\n\")\n",
    "    print_text(data)\n",
    "    conn.close()\n",
    "    return\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'application/octet-stream',\n",
    "        'Ocp-Apim-Subscription-Key': '',\n",
    "    }\n",
    "    params = urllib.parse.urlencode({\n",
    "        # Request parameters\n",
    "        'language': 'unk',\n",
    "        'detectOrientation ': 'true',\n",
    "    })\n",
    "    data = open('images/scannedImage.png', 'rb').read()\n",
    "    \n",
    "    try:\n",
    "        image_file = 'images/scannedImage.png'\n",
    "        im = Image.open(image_file)\n",
    "        im.show()\n",
    "        ocr_project_oxford(headers, params, data)\n",
    "    except Exception as e:\n",
    "        print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: Find contours of paper\n",
      "STEP 3: Apply perspective transform\n",
      "STEP 4: Apply Adaptive Threshold\n",
      "{\"language\":\"en\",\"textAngle\":0.0,\"orientation\":\"Up\",\"regions\":[{\"boundingBox\":\"27,121,185,212\",\"lines\":[{\"boundingBox\":\"27,121,183,45\",\"words\":[{\"boundingBox\":\"27,121,183,45\",\"text\":\"COFFEE\"}]},{\"boundingBox\":\"27,171,184,24\",\"words\":[{\"boundingBox\":\"27,171,184,24\",\"text\":\"COFFEEQUEUE\"}]},{\"boundingBox\":\"28,199,184,46\",\"words\":[{\"boundingBox\":\"28,199,184,46\",\"text\":\"COFFEE\"}]},{\"boundingBox\":\"29,248,183,52\",\"words\":[{\"boundingBox\":\"29,248,183,52\",\"text\":\"QUEUE\"}]},{\"boundingBox\":\"28,307,183,26\",\"words\":[{\"boundingBox\":\"28,307,183,26\",\"text\":\"COFFEEQUEUE\"}]}]}]}\n",
      "\n",
      "COFFEE\n",
      "COFFEEQUEUE\n",
      "COFFEE\n",
      "QUEUE\n",
      "COFFEEQUEUE\n"
     ]
    }
   ],
   "source": [
    "# 명함인식 구현하기 - 웹캠 + OCR\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64, json\n",
    "\n",
    "def print_text(json_data):\n",
    "    result = json.loads(json_data)\n",
    "    for l in result['regions']:\n",
    "        for w in l['lines']:\n",
    "            line = []\n",
    "            for r in w['words']:\n",
    "                line.append(r['text'])\n",
    "            print (' '.join(line))\n",
    "    return\n",
    "\n",
    "def ocr_project_oxford(headers, params, data):\n",
    "    conn = http.client.HTTPSConnection('westus.api.cognitive.microsoft.com')\n",
    "    conn.request(\"POST\", \"/vision/v1.0/ocr?%s\" % params, data, headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read().decode()\n",
    "    print (data + \"\\n\")\n",
    "    print_text(data)\n",
    "    conn.close()\n",
    "    return\n",
    "\n",
    "def order_points(pts):\n",
    "    # initialzie a list of coordinates that will be ordered\n",
    "    # such that the first entry in the list is the top-left,\n",
    "    # the second entry is the top-right, the third is the\n",
    "    # bottom-right, and the fourth is the bottom-left\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "\n",
    "    # the top-left point will have the smallest sum, whereas\n",
    "    # the bottom-right point will have the largest sum\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "\n",
    "    # now, compute the difference between the points, the\n",
    "    # top-right point will have the smallest difference,\n",
    "    # whereas the bottom-left will have the largest difference\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "\n",
    "    # return the ordered coordinates\n",
    "    return rect\n",
    "\n",
    "def auto_scan_image_via_webcam():\n",
    "    \n",
    "    try: \n",
    "        cap = cv2.VideoCapture(0)\n",
    "    except:\n",
    "        print ('cannot load camera')\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print ('cannot load camera!')\n",
    "            break\n",
    "            \n",
    "        k = cv2.waitKey(10)\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "        # convert the image to grayscale, blur it, and find edges\n",
    "        # in the image\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        edged = cv2.Canny(gray, 75, 200)\n",
    "\n",
    "        # show the original image and the edge detected image\n",
    "        # print (\"STEP 1: Edge Detection\")\n",
    "\n",
    "        # find the contours in the edged image, keeping only the\n",
    "        # largest ones, and initialize the screen contour\n",
    "        (_, cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "\n",
    "        # loop over the contours\n",
    "        for c in cnts:\n",
    "            # approximate the contour\n",
    "            peri = cv2.arcLength(c, True)\n",
    "            approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "            screenCnt = []\n",
    "\n",
    "            # if our approximated contour has four points, then we\n",
    "            # can assume that we have found our screen\n",
    "            if len(approx) == 4:\n",
    "                contourSize = cv2.contourArea(approx)\n",
    "                camSize = frame.shape[0] * frame.shape[1]\n",
    "                ratio = contourSize / camSize\n",
    "                # print (contourSize)\n",
    "                # print (camSize)\n",
    "                # print (ratio)\n",
    "                \n",
    "                if ratio > 0.1:\n",
    "                    screenCnt = approx\n",
    "                    \n",
    "                break \n",
    "        \n",
    "        if len(screenCnt) == 0:\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            # show the contour (outline) of the piece of paper\n",
    "            print (\"STEP 2: Find contours of paper\")\n",
    "\n",
    "            cv2.drawContours(frame, [screenCnt], -1, (0, 255, 0), 2)\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            \n",
    "            # apply the four point transform to obtain a top-down\n",
    "            # view of the original image\n",
    "            rect = order_points(screenCnt.reshape(4, 2))\n",
    "            (topLeft, topRight, bottomRight, bottomLeft) = rect\n",
    "\n",
    "            w1 = abs(bottomRight[0] - bottomLeft[0])\n",
    "            w2 = abs(topRight[0] - topLeft[0])\n",
    "            h1 = abs(topRight[1] - bottomRight[1])\n",
    "            h2 = abs(topLeft[1] - bottomLeft[1])\n",
    "            maxWidth = max([w1, w2])\n",
    "            maxHeight = max([h1, h2])\n",
    "\n",
    "            dst = np.float32([[0,0], [maxWidth-1,0], \n",
    "                              [maxWidth-1,maxHeight-1], [0,maxHeight-1]])\n",
    "\n",
    "            M = cv2.getPerspectiveTransform(rect, dst)\n",
    "            warped = cv2.warpPerspective(frame, M, (maxWidth, maxHeight))\n",
    "\n",
    "            # show the original and scanned images\n",
    "            print (\"STEP 3: Apply perspective transform\")\n",
    "\n",
    "            # convert the warped image to grayscale, then threshold it\n",
    "            # to give it that 'black and white' paper effect\n",
    "            warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "            warped = cv2.adaptiveThreshold(warped, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "\n",
    "            # show the original and scanned images\n",
    "            print (\"STEP 4: Apply Adaptive Threshold\")\n",
    "\n",
    "            break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "    cv2.imshow(\"Scanned\", warped)\n",
    "    cv2.imwrite('scannedImage.png', warped)\n",
    "    \n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'application/octet-stream',\n",
    "        'Ocp-Apim-Subscription-Key': '',\n",
    "    }\n",
    "    params = urllib.parse.urlencode({\n",
    "        # Request parameters\n",
    "        'language': 'unk',\n",
    "        'detectOrientation ': 'true',\n",
    "    })\n",
    "    data = open('scannedImage.png', 'rb').read()\n",
    "    \n",
    "    try:\n",
    "        image_file = 'scannedImage.png'\n",
    "        ocr_project_oxford(headers, params, data)\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    auto_scan_image_via_webcam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/byeondongnam/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:23: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/byeondongnam/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:27: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 548.0 -> 811.0 / 1161.0 (0.429037520392), 24339 -> 97293 / 356816 (2.9974115616911132), 0.626602103602 -> 0.712642158862\n",
      "1 811.0 -> 831.0 / 1161.0 (0.0571428571429), 97293 -> 97293 / 356816 (0.0), 0.712642158862 -> 0.721499799725\n",
      "(80, 80, 419, 367) -> (80, 80, 434, 367)\n",
      "(80, 80, 434, 367) -> (44, 80, 434, 367)\n",
      "(44, 80, 434, 367) -> (44, 80, 456, 367)\n",
      "images/scannedImage.png -> croppedImage.png\n"
     ]
    }
   ],
   "source": [
    "# (참고) OpenCV - 이미지에서 텍스트 영역만 찾아내기\n",
    "\n",
    "# 출처: http://www.danvk.org/2015/01/07/finding-blocks-of-text-in-an-image-using-python-opencv-and-numpy.html\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import rank_filter\n",
    "\n",
    "\n",
    "def dilate(ary, N, iterations):\n",
    "    \"\"\"Dilate using an NxN '+' sign shape. ary is np.uint8.\"\"\"\n",
    "    kernel = np.zeros((N,N), dtype=np.uint8)\n",
    "    kernel[(N-1)/2,:] = 1\n",
    "    dilated_image = cv2.dilate(ary / 255, kernel, iterations=iterations)\n",
    "\n",
    "    kernel = np.zeros((N,N), dtype=np.uint8)\n",
    "    kernel[:,(N-1)/2] = 1\n",
    "    dilated_image = cv2.dilate(dilated_image, kernel, iterations=iterations)\n",
    "    dilated_image = cv2.convertScaleAbs(dilated_image)\n",
    "    return dilated_image\n",
    "\n",
    "\n",
    "def props_for_contours(contours, ary):\n",
    "    \"\"\"Calculate bounding box & the number of set pixels for each contour.\"\"\"\n",
    "    c_info = []\n",
    "    for c in contours:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        c_im = np.zeros(ary.shape)\n",
    "        cv2.drawContours(c_im, [c], 0, 255, -1)\n",
    "        c_info.append({\n",
    "            'x1': x,\n",
    "            'y1': y,\n",
    "            'x2': x + w - 1,\n",
    "            'y2': y + h - 1,\n",
    "            'sum': np.sum(ary * (c_im > 0))/255\n",
    "        })\n",
    "    return c_info\n",
    "\n",
    "\n",
    "def union_crops(crop1, crop2):\n",
    "    \"\"\"Union two (x1, y1, x2, y2) rects.\"\"\"\n",
    "    x11, y11, x21, y21 = crop1\n",
    "    x12, y12, x22, y22 = crop2\n",
    "    return min(x11, x12), min(y11, y12), max(x21, x22), max(y21, y22)\n",
    "\n",
    "\n",
    "def intersect_crops(crop1, crop2):\n",
    "    x11, y11, x21, y21 = crop1\n",
    "    x12, y12, x22, y22 = crop2\n",
    "    return max(x11, x12), max(y11, y12), min(x21, x22), min(y21, y22)\n",
    "\n",
    "\n",
    "def crop_area(crop):\n",
    "    x1, y1, x2, y2 = crop\n",
    "    return max(0, x2 - x1) * max(0, y2 - y1)\n",
    "\n",
    "\n",
    "def find_border_components(contours, ary):\n",
    "    borders = []\n",
    "    area = ary.shape[0] * ary.shape[1]\n",
    "    for i, c in enumerate(contours):\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        if w * h > 0.5 * area:\n",
    "            borders.append((i, x, y, x + w - 1, y + h - 1))\n",
    "    return borders\n",
    "\n",
    "\n",
    "def angle_from_right(deg):\n",
    "    return min(deg % 90, 90 - (deg % 90))\n",
    "\n",
    "\n",
    "def remove_border(contour, ary):\n",
    "    \"\"\"Remove everything outside a border contour.\"\"\"\n",
    "    # Use a rotated rectangle (should be a good approximation of a border).\n",
    "    # If it's far from a right angle, it's probably two sides of a border and\n",
    "    # we should use the bounding box instead.\n",
    "    c_im = np.zeros(ary.shape)\n",
    "    r = cv2.minAreaRect(contour)\n",
    "    degs = r[2]\n",
    "    if angle_from_right(degs) <= 10.0:\n",
    "        box = cv2.boxPoints(r)\n",
    "        box = np.int0(box)\n",
    "        cv2.drawContours(c_im, [box], 0, 255, -1)\n",
    "        cv2.drawContours(c_im, [box], 0, 0, 4)\n",
    "    else:\n",
    "        x1, y1, x2, y2 = cv2.boundingRect(contour)\n",
    "        cv2.rectangle(c_im, (x1, y1), (x2, y2), 255, -1)\n",
    "        cv2.rectangle(c_im, (x1, y1), (x2, y2), 0, 4)\n",
    "\n",
    "    return np.minimum(c_im, ary)\n",
    "\n",
    "\n",
    "def find_components(edges, max_components=16):\n",
    "    \"\"\"Dilate the image until there are just a few connected components.\n",
    "    Returns contours for these components.\"\"\"\n",
    "    # Perform increasingly aggressive dilation until there are just a few\n",
    "    # connected components.\n",
    "    count = 21\n",
    "    dilation = 5\n",
    "    n = 1\n",
    "    while count > 16:\n",
    "        n += 1\n",
    "        dilated_image = dilate(edges, N=3, iterations=n)\n",
    "        _, contours, hierarchy = cv2.findContours(dilated_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        count = len(contours)\n",
    "    #print dilation\n",
    "    #Image.fromarray(edges).show()\n",
    "    #Image.fromarray(255 * dilated_image).show()\n",
    "    return contours\n",
    "\n",
    "\n",
    "def find_optimal_components_subset(contours, edges):\n",
    "    \"\"\"Find a crop which strikes a good balance of coverage/compactness.\n",
    "    Returns an (x1, y1, x2, y2) tuple.\n",
    "    \"\"\"\n",
    "    c_info = props_for_contours(contours, edges)\n",
    "    c_info.sort(key=lambda x: -x['sum'])\n",
    "    total = np.sum(edges) / 255\n",
    "    area = edges.shape[0] * edges.shape[1]\n",
    "\n",
    "    c = c_info[0]\n",
    "    del c_info[0]\n",
    "    this_crop = c['x1'], c['y1'], c['x2'], c['y2']\n",
    "    crop = this_crop\n",
    "    covered_sum = c['sum']\n",
    "\n",
    "    while covered_sum < total:\n",
    "        changed = False\n",
    "        recall = 1.0 * covered_sum / total\n",
    "        prec = 1 - 1.0 * crop_area(crop) / area\n",
    "        f1 = 2 * (prec * recall / (prec + recall))\n",
    "        #print '----'\n",
    "        for i, c in enumerate(c_info):\n",
    "            this_crop = c['x1'], c['y1'], c['x2'], c['y2']\n",
    "            new_crop = union_crops(crop, this_crop)\n",
    "            new_sum = covered_sum + c['sum']\n",
    "            new_recall = 1.0 * new_sum / total\n",
    "            new_prec = 1 - 1.0 * crop_area(new_crop) / area\n",
    "            new_f1 = 2 * new_prec * new_recall / (new_prec + new_recall)\n",
    "\n",
    "            # Add this crop if it improves f1 score,\n",
    "            # _or_ it adds 25% of the remaining pixels for <15% crop expansion.\n",
    "            # ^^^ very ad-hoc! make this smoother\n",
    "            remaining_frac = c['sum'] / (total - covered_sum)\n",
    "            new_area_frac = 1.0 * crop_area(new_crop) / crop_area(crop) - 1\n",
    "            if new_f1 > f1 or (\n",
    "                    remaining_frac > 0.25 and new_area_frac < 0.15):\n",
    "                print('%d %s -> %s / %s (%s), %s -> %s / %s (%s), %s -> %s' % (\n",
    "                        i, covered_sum, new_sum, total, remaining_frac,\n",
    "                        crop_area(crop), crop_area(new_crop), area, new_area_frac,\n",
    "                        f1, new_f1))\n",
    "                crop = new_crop\n",
    "                covered_sum = new_sum\n",
    "                del c_info[i]\n",
    "                changed = True\n",
    "                break\n",
    "\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "    return crop\n",
    "\n",
    "\n",
    "def pad_crop(crop, contours, edges, border_contour, pad_px=15):\n",
    "    \"\"\"Slightly expand the crop to get full contours.\n",
    "    This will expand to include any contours it currently intersects, but will\n",
    "    not expand past a border.\n",
    "    \"\"\"\n",
    "    bx1, by1, bx2, by2 = 0, 0, edges.shape[0], edges.shape[1]\n",
    "    if border_contour is not None and len(border_contour) > 0:\n",
    "        c = props_for_contours([border_contour], edges)[0]\n",
    "        bx1, by1, bx2, by2 = c['x1'] + 5, c['y1'] + 5, c['x2'] - 5, c['y2'] - 5\n",
    "\n",
    "    def crop_in_border(crop):\n",
    "        x1, y1, x2, y2 = crop\n",
    "        x1 = max(x1 - pad_px, bx1)\n",
    "        y1 = max(y1 - pad_px, by1)\n",
    "        x2 = min(x2 + pad_px, bx2)\n",
    "        y2 = min(y2 + pad_px, by2)\n",
    "        return crop\n",
    "\n",
    "    crop = crop_in_border(crop)\n",
    "\n",
    "    c_info = props_for_contours(contours, edges)\n",
    "    changed = False\n",
    "    for c in c_info:\n",
    "        this_crop = c['x1'], c['y1'], c['x2'], c['y2']\n",
    "        this_area = crop_area(this_crop)\n",
    "        int_area = crop_area(intersect_crops(crop, this_crop))\n",
    "        new_crop = crop_in_border(union_crops(crop, this_crop))\n",
    "        if 0 < int_area < this_area and crop != new_crop:\n",
    "            print('%s -> %s' % (str(crop), str(new_crop)))\n",
    "            changed = True\n",
    "            crop = new_crop\n",
    "\n",
    "    if changed:\n",
    "        return pad_crop(crop, contours, edges, border_contour, pad_px)\n",
    "    else:\n",
    "        return crop\n",
    "\n",
    "\n",
    "def downscale_image(im, max_dim=2048):\n",
    "    \"\"\"Shrink im until its longest dimension is <= max_dim.\n",
    "    Returns new_image, scale (where scale <= 1).\n",
    "    \"\"\"\n",
    "    a = im.shape[0]\n",
    "    b = im.shape[1]\n",
    "    if max(a, b) <= max_dim:\n",
    "        return 1.0, im\n",
    "\n",
    "    scale = 1.0 * max_dim / max(a, b)\n",
    "    dim = (int(a * scale), int(b * scale))\n",
    "    new_im = cv2.resize(im, dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    return scale, new_im\n",
    "\n",
    "\n",
    "def process_image(path, out_path):\n",
    "    orig_im = Image.open(path)\n",
    "    im = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    scale, im = downscale_image(im)\n",
    "\n",
    "    edges = cv2.Canny(im, 100, 200)\n",
    "\n",
    "    # TODO: dilate image _before_ finding a border. This is crazy sensitive!\n",
    "    _, contours, hierarchy = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    borders = find_border_components(contours, edges)\n",
    "    borders.sort(key=lambda i_x1_y1_x2_y2: (i_x1_y1_x2_y2[3] - i_x1_y1_x2_y2[1]) * (i_x1_y1_x2_y2[4] - i_x1_y1_x2_y2[2]))\n",
    "\n",
    "    border_contour = None\n",
    "    if len(borders):\n",
    "        border_contour = contours[borders[0][0]]\n",
    "        edges = remove_border(border_contour, edges)\n",
    "\n",
    "    edges = 255 * (edges > 0).astype(np.uint8)\n",
    "\n",
    "    # Remove ~1px borders using a rank filter.\n",
    "    maxed_rows = rank_filter(edges, -5, size=(1, 20))\n",
    "    maxed_cols = rank_filter(edges, -5, size=(20, 1))\n",
    "    debordered = np.minimum(np.minimum(edges, maxed_rows), maxed_cols)\n",
    "    edges = debordered\n",
    "\n",
    "    contours = find_components(edges)\n",
    "    if len(contours) == 0:\n",
    "        print('%s -> (no text!)' % path)\n",
    "        return\n",
    "\n",
    "    crop = find_optimal_components_subset(contours, edges)\n",
    "    crop = pad_crop(crop, contours, edges, border_contour)\n",
    "\n",
    "    crop = [int(x / scale) for x in crop]  # upscale to the original image size.\n",
    "\n",
    "    # draw and show cropped rectangle area in the original image\n",
    "    rgb_im = orig_im.convert('RGB')\n",
    "    draw = ImageDraw.Draw(rgb_im)\n",
    "    draw.rectangle(crop, outline='red')\n",
    "    rgb_im.show()\n",
    "\n",
    "    text_im = orig_im.crop(crop)\n",
    "    text_im.show()\n",
    "    text_im.save(out_path)\n",
    "    print('%s -> %s' % (path, out_path))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # path = 'images/text.jpg'\n",
    "    path = 'images/scannedImage.png'\n",
    "    out_path = 'croppedImage.png'\n",
    "    try:\n",
    "        process_image(path, out_path)\n",
    "    except Exception as e:\n",
    "        print('%s %s' % (path, e))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
